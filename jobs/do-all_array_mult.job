#!/bin/bash

#SBATCH --partition=gpu_titanrtx_shared_course
#SBATCH --gres=gpu:1
#SBATCH --job-name=DoAllMultNLI
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --time=04:00:00
#SBATCH --mem=32000M
#SBATCH --array=1-4%2
#SBATCH --output=jobs/slurm_output/do-all-mult_%A_%a.out

module purge
module load 2021
module load Anaconda3/2021.05

ARRAY_FILE=$HOME/NLI2/jobs/model_types_mult.txt

cd $HOME/NLI2/
source activate nli2

srun python -u nli/train.py   $(head -$SLURM_ARRAY_TASK_ID $ARRAY_FILE | tail -1)
srun python -u nli/results.py $(head -$SLURM_ARRAY_TASK_ID $ARRAY_FILE | tail -1) --transfer_results
